{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
      "<html xmlns=\"http://www.w3.\n"
     ]
    }
   ],
   "source": [
    "html_doc = res.text\n",
    "print(html_doc[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_house_detail = soup.find('dl', class_='racedata fc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¥Õ¥§¥Ö¥é¥ê¡¼£Ó\\xa0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_name = div_house_detail.find('h1')\n",
    "race_name.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n11R\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_round = div_house_detail.find('dt')\n",
    "race_round.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¥À1600m\\xa0(º¸)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_detail = div_house_detail.find('p')\n",
    "race_detail.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11R'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_round = div_house_detail.find('dt')\n",
    "race_round.get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¥À1600m\\xa0(º¸)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_detail = div_house_detail.find('p')\n",
    "race_detail.get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d6a686714c94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrace_detail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiv_house_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrace_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \"\"\"tag[key] returns the value of the 'key' attribute for the tag,\n\u001b[0;32m   1070\u001b[0m         and throws an exception if it's not there.\"\"\"\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "race_detail = div_house_detail.find('p')[1]\n",
    "race_detail.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-03f68cc9bfe6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrace_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiv_house_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrace_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "race_name = div_house_detail.find('h1').content\n",
    "race_name.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f839be93badb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrace_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiv_house_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrace_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "race_name = div_house_detail.find('h1')\n",
    "race_name.get_text().content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¥Õ¥§¥Ö¥é¥ê¡¼£Ó'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_name = div_house_detail.find('h1')\n",
    "race_name.get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-070d83177a69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrace_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiv_house_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrace_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'context'"
     ]
    }
   ],
   "source": [
    "race_name = div_house_detail.find('h1')\n",
    "race_name.get_text().context[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a0ef2cfabfe1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrace_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiv_house_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrace_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'content'"
     ]
    }
   ],
   "source": [
    "import content\n",
    "race_name = div_house_detail.find('h1')\n",
    "race_name.get_text().context[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'div_house_detail' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-616af83e9551>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrace_detail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiv_house_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrace_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'div_house_detail' is not defined"
     ]
    }
   ],
   "source": [
    "race_detail = div_house_detail.find('p')\n",
    "race_detail.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', class_='race_table_01nk_tb_common')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(results_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', class_='race_table_01nk_tb_common')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(results_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', class_='race_table_01nk_tb_common')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: value[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(results_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', class_='race_table_01nk_tb_common')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(results_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', class_='common')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(results_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a6e3a8f128db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mres_tables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'common'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres_table\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res_table' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', class_='common')\n",
    "\n",
    "for table in res_table:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(results_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', c_='common')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(results_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', summy_='レース結果')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(results_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', class_='common')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(results_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:26<03:58, 26.55s/it]\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:50<03:26, 25.85s/it]\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [01:15<02:58, 25.49s/it]\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [01:40<02:31, 25.25s/it]\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [02:03<02:03, 24.78s/it]\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [02:27<01:38, 24.55s/it]\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [02:55<01:16, 25.57s/it]\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [03:20<00:50, 25.31s/it]\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [03:21<00:18, 18.09s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [03:22<00:00, 13.03s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [03:22<30:26, 202.93s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:26<03:56, 26.25s/it]\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:50<03:24, 25.56s/it]\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [01:13<02:55, 25.02s/it]\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [01:37<02:27, 24.61s/it]\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [02:00<02:00, 24.10s/it]\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [02:24<01:35, 23.93s/it]\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [02:50<01:13, 24.62s/it]\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [03:24<00:55, 27.52s/it]\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [03:25<00:19, 19.62s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [03:26<00:00, 14.10s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [06:49<27:13, 204.15s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:01<00:10,  1.18s/it]\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:02<00:09,  1.18s/it]\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:03<00:08,  1.17s/it]\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [00:04<00:07,  1.19s/it]\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.18s/it]\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:07<00:04,  1.17s/it]\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.17s/it]\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.16s/it]\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.16s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.16s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [07:01<17:04, 146.41s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:01<00:10,  1.16s/it]\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:02<00:09,  1.16s/it]\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:03<00:08,  1.16s/it]\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [00:04<00:07,  1.17s/it]\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.18s/it]\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:07<00:04,  1.17s/it]\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.18s/it]\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.18s/it]\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.18s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [07:13<10:36, 106.02s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:01<00:10,  1.17s/it]\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:02<00:09,  1.20s/it]\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:03<00:08,  1.19s/it]\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [00:04<00:07,  1.18s/it]\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.18s/it]\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:07<00:04,  1.17s/it]\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.18s/it]\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.18s/it]\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.21s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.19s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [07:25<06:28, 77.79s/it] \n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:01<00:10,  1.16s/it]\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:02<00:09,  1.16s/it]\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:03<00:08,  1.18s/it]\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [00:04<00:07,  1.19s/it]\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.19s/it]\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:07<00:04,  1.18s/it]\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.17s/it]\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.18s/it]\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.17s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [07:37<03:51, 57.99s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:01<00:10,  1.18s/it]\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:02<00:09,  1.18s/it]\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:03<00:09,  1.29s/it]\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [00:05<00:07,  1.24s/it]\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:06<00:06,  1.27s/it]\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:07<00:04,  1.25s/it]\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.23s/it]\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.21s/it]\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [00:11<00:01,  1.20s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.19s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [07:49<02:12, 44.28s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:01<00:10,  1.17s/it]\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:02<00:09,  1.16s/it]\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:03<00:08,  1.17s/it]\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [00:04<00:07,  1.18s/it]\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:05<00:06,  1.20s/it]\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:07<00:04,  1.22s/it]\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.23s/it]\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.21s/it]\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.19s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.19s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [08:01<01:09, 34.58s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:01<00:10,  1.19s/it]\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:02<00:09,  1.19s/it]\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:03<00:08,  1.19s/it]\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [00:04<00:07,  1.20s/it]\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.19s/it]\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:07<00:04,  1.18s/it]\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.20s/it]\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.19s/it]\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.19s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.19s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [08:13<00:27, 27.79s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:01<00:14,  1.59s/it]\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:02<00:11,  1.47s/it]\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:03<00:09,  1.39s/it]\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [00:05<00:07,  1.33s/it]\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:06<00:06,  1.29s/it]\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:07<00:05,  1.27s/it]\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [00:08<00:03,  1.23s/it]\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.21s/it]\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [00:11<00:01,  1.20s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.20s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [08:25<00:00, 23.13s/it]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [08:25<1:15:49, 505.54s/it]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:23<03:32, 23.67s/it]\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:47<03:09, 23.71s/it]\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [01:11<02:46, 23.76s/it]\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [01:35<02:22, 23.76s/it]\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [02:00<02:00, 24.14s/it]\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [02:24<01:37, 24.33s/it]\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [02:51<01:14, 24.91s/it]\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [03:15<00:49, 24.71s/it]\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [03:16<00:17, 17.66s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [03:17<00:00, 12.71s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [03:17<29:40, 197.83s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:23<03:32, 23.66s/it]\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:48<03:11, 23.97s/it]\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [01:13<02:50, 24.31s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-74675c65508d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m                     \u001b[1;31m# urlでぶっこ抜く\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBase\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumStr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumStr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumStr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumStr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                     \u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'EUC-JP'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def numStr(num):\n",
    "    if num >= 10:\n",
    "        return str(num)\n",
    "    else:\n",
    "        return '0' + str(num)\n",
    "\n",
    "Base = \"http://race.sp.netkeiba.com/?pid=race_result&race_id=\"\n",
    "dst = ''\n",
    "df_col = ['year', 'date', 'field', 'race', 'race_name'\n",
    "          , 'course', 'head_count', 'rank', 'horse_name'\n",
    "          , 'gender', 'age', 'trainerA', 'trainerB', 'weight', 'c_weight', 'jackie', 'j_weight'\n",
    "          , 'odds','popu']\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for year in tqdm(range(2008, 2019)):\n",
    "    for i in tqdm(range(1, 11)):\n",
    "        for j in tqdm(range(1, 11)):\n",
    "            for k in tqdm(range(1, 11)):\n",
    "                for l in range(1, 13):\n",
    "                    # urlでぶっこ抜く\n",
    "                    url = Base + str(year) + numStr(i) + numStr(j) + numStr(k) + numStr(l)\n",
    "                    time.sleep(1)\n",
    "                    html = requests.get(url)\n",
    "                    html.encoding = 'EUC-JP'\n",
    "\n",
    "                    # scraping\n",
    "                    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "                    # ページがあるかの判定\n",
    "                    if soup.find_all('div', attrs={'class', 'Result_Guide'})!=[]:\n",
    "                        break\n",
    "                    else:\n",
    "                        #共通部分を抜き出す\n",
    "                        CommonYear = year\n",
    "                        CommonDate = soup.find_all('div', attrs={'class', 'Change_Btn Day'})[0].string.strip()\n",
    "                        CommonField= soup.find_all('div', attrs={'class', 'Change_Btn Course'})[0].string.strip()\n",
    "                        CommonRace = soup.find_all('div', attrs={'Race_Num'})[0].span.string\n",
    "                        CommonRname= soup.find_all('dt', attrs={'class', 'Race_Name'})[0].contents[0].strip()\n",
    "                        CommonCourse= soup.find_all('dd', attrs={'Race_Data'})[0].span.string\n",
    "                        CommonHcount= soup.find_all('dd', attrs={'class', 'Race_Data'})[0].contents[3].split()[1]\n",
    "\n",
    "                        for m in range(len(soup.find_all('div', attrs='Rank'))):\n",
    "                            dst = pd.Series(index=df_col)\n",
    "                            try:\n",
    "                                dst['year'] = CommonYear\n",
    "                                dst['date'] = CommonDate\n",
    "                                dst['field']= CommonField #開催場所\n",
    "                                dst['race'] = CommonRace\n",
    "                                dst['race_name'] = CommonRname\n",
    "                                dst['course'] = CommonCourse\n",
    "                                dst['head_count'] = CommonHcount #頭数\n",
    "                                dst['rank'] = soup.find_all('div', attrs='Rank')[m].contents[0]\n",
    "                                dst['horse_name'] = soup.find_all('dt', attrs=['class', 'Horse_Name'])[m].a.string\n",
    "                                detailL = soup.find_all('span', attrs=['class', 'Detail_Left'])[m]\n",
    "                                dst['gender'] = list(detailL.contents[0].split()[0])[0]\n",
    "                                dst['age'] = list(detailL.contents[0].split()[0])[1]\n",
    "                                dst['trainerA'] = detailL.span.string.split('･')[0]\n",
    "                                dst['trainerB'] = detailL.span.string.split('･')[1]\n",
    "                                if len(detailL.contents[0].split())>=2:\n",
    "                                    dst['weight'] = detailL.contents[0].split()[1].split('(')[0]\n",
    "                                    if len(detailL.contents[0].split()[1].split('('))>=2:\n",
    "                                        dst['c_weight'] = detailL.contents[0].split()[1].split('(')[1].strip(')') #多分馬の体重変動\n",
    "                                detailR = soup.find_all('span', attrs=['class', 'Detail_Right'])[m].contents\n",
    "                                if  \"\\n\" in detailR or \"\\n▲\" in detailR or '\\n☆' in detailR:\n",
    "                                    detailR.pop(0)\n",
    "                                dst['jackie'] = detailR[0].string.strip()\n",
    "                                dst['j_weight'] = detailR[2].strip().replace('(', '').replace(')', '') #多分jackieの体重変動\n",
    "                                Odds = soup.find_all('td', attrs=['class', 'Odds'])[m].contents[1]\n",
    "                                if Odds.dt.string is not None:\n",
    "                                    dst['odds'] = Odds.dt.string.strip('倍')\n",
    "                                    dst['popu'] = Odds.dd.string.strip('人気') #何番人気か\n",
    "                            except:\n",
    "                                pass\n",
    "                            dst.name = str(year) + numStr(i) + numStr(j) + numStr(k) + numStr(l) + numStr(m)\n",
    "\n",
    "                            df = df.append(dst)\n",
    "\n",
    "df.to_csv('keiba_PS.csv', encoding='shift-jis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f2816f5c3a32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m             result_dict = {headers[i]: values[i] for i in\n\u001b[0;32m     24\u001b[0m             range(len(values))}\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_dict' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', class_='race_table_01')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(results_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f2816f5c3a32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m             result_dict = {headers[i]: values[i] for i in\n\u001b[0;32m     24\u001b[0m             range(len(values))}\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_dict' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', class_='race_table_01')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(result_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ãå½ç': '1', 'ÏÈÈÖ': '4', 'ÇÏÈÖ': '6', 'ÇÏÌ¾': ' ¥¤¥ó¥Æ¥£', 'À\\xadÎð': '²´5', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' ÉðË\\xad', '¥¿¥¤¥à': '1:35.6', 'Ãåº¹': '', '¿Íµ¤': '1', 'Ã±¾¡¥ª¥Ã¥º': '2.6', '¸å3F': '35.4', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '1-1', '±¹¼Ë': '(·ªÅì)ÌîÃæ', 'ÇÏÂÎ½Å': '514(0)'}\n",
      "{'Ãå½ç': '2', 'ÏÈÈÖ': '3', 'ÇÏÈÖ': '3', 'ÇÏÌ¾': ' ¥´¡¼¥ë¥É¥É¥ê¡¼¥à', 'À\\xadÎð': '²´6', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' ¥ë¥á¡¼¥ë', '¥¿¥¤¥à': '1:35.6', 'Ãåº¹': '¥¯¥Ó', '¿Íµ¤': '2', 'Ã±¾¡¥ª¥Ã¥º': '3.0', '¸å3F': '34.8', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '6-6', '±¹¼Ë': '(·ªÅì)Ê¿ÅÄ', 'ÇÏÂÎ½Å': '528(+14)'}\n",
      "{'Ãå½ç': '3', 'ÏÈÈÖ': '2', 'ÇÏÈÖ': '2', 'ÇÏÌ¾': ' ¥æ¥é¥Î¥È', 'À\\xadÎð': '²´5', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' Ê¡±Ê', '¥¿¥¤¥à': '1:36.3', 'Ãåº¹': '4', '¿Íµ¤': '8', 'Ã±¾¡¥ª¥Ã¥º': '31.6', '¸å3F': '35.5', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '6-6', '±¹¼Ë': '(·ªÅì)¾¾ÅÄ', 'ÇÏÂÎ½Å': '508(+2)'}\n",
      "{'Ãå½ç': '4', 'ÏÈÈÖ': '5', 'ÇÏÈÖ': '8', 'ÇÏÌ¾': '¡û³°¥â¡¼¥Ë¥ó', 'À\\xadÎð': '²´7', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' ÏÂÅÄ', '¥¿¥¤¥à': '1:36.5', 'Ãåº¹': '1.1/4', '¿Íµ¤': '9', 'Ã±¾¡¥ª¥Ã¥º': '42.9', '¸å3F': '35.9', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '5-3', '±¹¼Ë': '(·ªÅì)ÀÐºäÀµ', 'ÇÏÂÎ½Å': '518(-4)'}\n",
      "{'Ãå½ç': '5', 'ÏÈÈÖ': '7', 'ÇÏÈÖ': '11', 'ÇÏÌ¾': '¡û³°¥³¥Ñ¥Î¥\\xad¥Ã¥\\xad¥ó¥°', 'À\\xadÎð': '¥»4', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' Æ£ÅÄºÚ', '¥¿¥¤¥à': '1:36.6', 'Ãåº¹': '1/2', '¿Íµ¤': '4', 'Ã±¾¡¥ª¥Ã¥º': '9.4', '¸å3F': '35.2', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '13-14', '±¹¼Ë': '(·ªÅì)Â¼»³', 'ÇÏÂÎ½Å': '478(0)'}\n",
      "{'Ãå½ç': '6', 'ÏÈÈÖ': '6', 'ÇÏÈÖ': '10', 'ÇÏÌ¾': ' ¥µ¥ó¥é¥¤¥º¥½¥¢', 'À\\xadÎð': '²´5', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' ÅÄÊÕ', '¥¿¥¤¥à': '1:36.7', 'Ãåº¹': '¥¯¥Ó', '¿Íµ¤': '5', 'Ã±¾¡¥ª¥Ã¥º': '14.1', '¸å3F': '36.3', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '2-2', '±¹¼Ë': '(·ªÅì)²ÏÆâ', 'ÇÏÂÎ½Å': '504(-18)'}\n",
      "{'Ãå½ç': '7', 'ÏÈÈÖ': '5', 'ÇÏÈÖ': '7', 'ÇÏÌ¾': ' ¥µ¥ó¥é¥¤¥º¥Î¥ô¥¡', 'À\\xadÎð': '²´5', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' ¸Íºê·½', '¥¿¥¤¥à': '1:36.9', 'Ãåº¹': '1.1/4', '¿Íµ¤': '6', 'Ã±¾¡¥ª¥Ã¥º': '16.7', '¸å3F': '35.7', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '10-10', '±¹¼Ë': '(·ªÅì)²»Ìµ', 'ÇÏÂÎ½Å': '546(+6)'}\n",
      "{'Ãå½ç': '8', 'ÏÈÈÖ': '4', 'ÇÏÈÖ': '5', 'ÇÏÌ¾': ' ¥µ¥¯¥»¥¹¥¨¥Ê¥¸¡¼', 'À\\xadÎð': '²´5', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' ¾¾»³', '¥¿¥¤¥à': '1:37.1', 'Ãåº¹': '1', '¿Íµ¤': '11', 'Ã±¾¡¥ª¥Ã¥º': '95.0', '¸å3F': '36.5', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '3-3', '±¹¼Ë': '(·ªÅì)ËÌ½Ð', 'ÇÏÂÎ½Å': '542(0)'}\n",
      "{'Ãå½ç': '9', 'ÏÈÈÖ': '6', 'ÇÏÈÖ': '9', 'ÇÏÌ¾': ' ¥ï¥ó¥À¡¼¥ê¡¼¥Ç¥ë', 'À\\xadÎð': '²´6', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' ¼ÆÅÄÂç', '¥¿¥¤¥à': '1:37.2', 'Ãåº¹': '1/2', '¿Íµ¤': '14', 'Ã±¾¡¥ª¥Ã¥º': '429.0', '¸å3F': '36.6', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '3-3', '±¹¼Ë': '(·ªÅì)²\\xad', 'ÇÏÂÎ½Å': '530(-4)'}\n",
      "{'Ãå½ç': '10', 'ÏÈÈÖ': '8', 'ÇÏÈÖ': '14', 'ÇÏÌ¾': ' ¥ª¥á¥¬¥Ñ¥Õ¥å¡¼¥à', 'À\\xadÎð': '²´4', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' £Í¥Ç¥à¡¼¥í', '¥¿¥¤¥à': '1:37.3', 'Ãåº¹': '3/4', '¿Íµ¤': '3', 'Ã±¾¡¥ª¥Ã¥º': '6.2', '¸å3F': '36.4', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '6-8', '±¹¼Ë': '(·ªÅì)°ÂÅÄæÆ', 'ÇÏÂÎ½Å': '450(+2)'}\n",
      "{'Ãå½ç': '11', 'ÏÈÈÖ': '1', 'ÇÏÈÖ': '1', 'ÇÏÌ¾': ' ¥¯¥¤¥ó¥º¥µ¥¿¡¼¥ó', 'À\\xadÎð': '²´6', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' »Í°Ì', '¥¿¥¤¥à': '1:37.4', 'Ãåº¹': '3/4', '¿Íµ¤': '10', 'Ã±¾¡¥ª¥Ã¥º': '61.3', '¸å3F': '36.2', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '13-10', '±¹¼Ë': '(·ªÅì)ÌîÃæ', 'ÇÏÂÎ½Å': '476(0)'}\n",
      "{'Ãå½ç': '12', 'ÏÈÈÖ': '7', 'ÇÏÈÖ': '12', 'ÇÏÌ¾': ' ¥Î¥Ü¥Ð¥«¥é', 'À\\xadÎð': '²´7', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' ¥ß¥Ê¥ê¥¯', '¥¿¥¤¥à': '1:37.8', 'Ãåº¹': '2', '¿Íµ¤': '12', 'Ã±¾¡¥ª¥Ã¥º': '338.1', '¸å3F': '36.6', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '10-10', '±¹¼Ë': '(·ªÅì)¿¹', 'ÇÏÂÎ½Å': '510(+4)'}\n",
      "{'Ãå½ç': '13', 'ÏÈÈÖ': '8', 'ÇÏÈÖ': '13', 'ÇÏÌ¾': ' ¥Î¥ó¥³¥Î¥æ¥á', 'À\\xadÎð': '¥»7', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' ÆâÅÄÇî', '¥¿¥¤¥à': '1:37.9', 'Ãåº¹': '1/2', '¿Íµ¤': '7', 'Ã±¾¡¥ª¥Ã¥º': '17.9', '¸å3F': '36.9', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '6-8', '±¹¼Ë': '(Èþ±º)²ÃÆ£À¬', 'ÇÏÂÎ½Å': '452(+6)'}\n",
      "{'Ãå½ç': '14', 'ÏÈÈÖ': '3', 'ÇÏÈÖ': '4', 'ÇÏÌ¾': ' ¥á¥¤¥·¥ç¥¦¥¦¥¿¥²', 'À\\xadÎð': '²´8', 'ÉéÃ´½ÅÎÌ': '57.0', 'µ³¼ê': ' ËÌÂ¼¹¨', '¥¿¥¤¥à': '1:37.9', 'Ãåº¹': '¥¯¥Ó', '¿Íµ¤': '13', 'Ã±¾¡¥ª¥Ã¥º': '393.4', '¸å3F': '36.7', '¥³¡¼¥Ê¡¼ÄÌ²á½ç': '10-10', '±¹¼Ë': '(·ªÅì)°ÂÃ£', 'ÇÏÂÎ½Å': '520(0)'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', class_='race_table_01')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(result_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'着順': '1', '枠番': '4', '馬番': '6', '馬名': ' インティ', '性齢': '牡5', '負担重量': '57.0', '騎手': ' 武豊', 'タイム': '1:35.6', '着差': '', '人気': '1', '単勝オッズ': '2.6', '後3F': '35.4', 'コーナー通過順': '1-1', '厩舎': '(栗東)野中', '馬体重': '514(0)'}\n",
      "{'着順': '2', '枠番': '3', '馬番': '3', '馬名': ' ゴールドドリーム', '性齢': '牡6', '負担重量': '57.0', '騎手': ' ルメール', 'タイム': '1:35.6', '着差': 'クビ', '人気': '2', '単勝オッズ': '3.0', '後3F': '34.8', 'コーナー通過順': '6-6', '厩舎': '(栗東)平田', '馬体重': '528(+14)'}\n",
      "{'着順': '3', '枠番': '2', '馬番': '2', '馬名': ' ユラノト', '性齢': '牡5', '負担重量': '57.0', '騎手': ' 福永', 'タイム': '1:36.3', '着差': '4', '人気': '8', '単勝オッズ': '31.6', '後3F': '35.5', 'コーナー通過順': '6-6', '厩舎': '(栗東)松田', '馬体重': '508(+2)'}\n",
      "{'着順': '4', '枠番': '5', '馬番': '8', '馬名': '○外モーニン', '性齢': '牡7', '負担重量': '57.0', '騎手': ' 和田', 'タイム': '1:36.5', '着差': '1.1/4', '人気': '9', '単勝オッズ': '42.9', '後3F': '35.9', 'コーナー通過順': '5-3', '厩舎': '(栗東)石坂正', '馬体重': '518(-4)'}\n",
      "{'着順': '5', '枠番': '7', '馬番': '11', '馬名': '○外コパノキッキング', '性齢': 'セ4', '負担重量': '57.0', '騎手': ' 藤田菜', 'タイム': '1:36.6', '着差': '1/2', '人気': '4', '単勝オッズ': '9.4', '後3F': '35.2', 'コーナー通過順': '13-14', '厩舎': '(栗東)村山', '馬体重': '478(0)'}\n",
      "{'着順': '6', '枠番': '6', '馬番': '10', '馬名': ' サンライズソア', '性齢': '牡5', '負担重量': '57.0', '騎手': ' 田辺', 'タイム': '1:36.7', '着差': 'クビ', '人気': '5', '単勝オッズ': '14.1', '後3F': '36.3', 'コーナー通過順': '2-2', '厩舎': '(栗東)河内', '馬体重': '504(-18)'}\n",
      "{'着順': '7', '枠番': '5', '馬番': '7', '馬名': ' サンライズノヴァ', '性齢': '牡5', '負担重量': '57.0', '騎手': ' 戸崎圭', 'タイム': '1:36.9', '着差': '1.1/4', '人気': '6', '単勝オッズ': '16.7', '後3F': '35.7', 'コーナー通過順': '10-10', '厩舎': '(栗東)音無', '馬体重': '546(+6)'}\n",
      "{'着順': '8', '枠番': '4', '馬番': '5', '馬名': ' サクセスエナジー', '性齢': '牡5', '負担重量': '57.0', '騎手': ' 松山', 'タイム': '1:37.1', '着差': '1', '人気': '11', '単勝オッズ': '95.0', '後3F': '36.5', 'コーナー通過順': '3-3', '厩舎': '(栗東)北出', '馬体重': '542(0)'}\n",
      "{'着順': '9', '枠番': '6', '馬番': '9', '馬名': ' ワンダーリーデル', '性齢': '牡6', '負担重量': '57.0', '騎手': ' 柴田大', 'タイム': '1:37.2', '着差': '1/2', '人気': '14', '単勝オッズ': '429.0', '後3F': '36.6', 'コーナー通過順': '3-3', '厩舎': '(栗東)沖', '馬体重': '530(-4)'}\n",
      "{'着順': '10', '枠番': '8', '馬番': '14', '馬名': ' オメガパフューム', '性齢': '牡4', '負担重量': '57.0', '騎手': ' Ｍデムーロ', 'タイム': '1:37.3', '着差': '3/4', '人気': '3', '単勝オッズ': '6.2', '後3F': '36.4', 'コーナー通過順': '6-8', '厩舎': '(栗東)安田翔', '馬体重': '450(+2)'}\n",
      "{'着順': '11', '枠番': '1', '馬番': '1', '馬名': ' クインズサターン', '性齢': '牡6', '負担重量': '57.0', '騎手': ' 四位', 'タイム': '1:37.4', '着差': '3/4', '人気': '10', '単勝オッズ': '61.3', '後3F': '36.2', 'コーナー通過順': '13-10', '厩舎': '(栗東)野中', '馬体重': '476(0)'}\n",
      "{'着順': '12', '枠番': '7', '馬番': '12', '馬名': ' ノボバカラ', '性齢': '牡7', '負担重量': '57.0', '騎手': ' ミナリク', 'タイム': '1:37.8', '着差': '2', '人気': '12', '単勝オッズ': '338.1', '後3F': '36.6', 'コーナー通過順': '10-10', '厩舎': '(栗東)森', '馬体重': '510(+4)'}\n",
      "{'着順': '13', '枠番': '8', '馬番': '13', '馬名': ' ノンコノユメ', '性齢': 'セ7', '負担重量': '57.0', '騎手': ' 内田博', 'タイム': '1:37.9', '着差': '1/2', '人気': '7', '単勝オッズ': '17.9', '後3F': '36.9', 'コーナー通過順': '6-8', '厩舎': '(美浦)加藤征', '馬体重': '452(+6)'}\n",
      "{'着順': '14', '枠番': '3', '馬番': '4', '馬名': ' メイショウウタゲ', '性齢': '牡8', '負担重量': '57.0', '騎手': ' 北村宏', 'タイム': '1:37.9', '着差': 'クビ', '人気': '13', '単勝オッズ': '393.4', '後3F': '36.7', 'コーナー通過順': '10-10', '厩舎': '(栗東)安達', '馬体重': '520(0)'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.content\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', class_='race_table_01')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(result_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-37a64d4fd54d>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-37a64d4fd54d>\"\u001b[1;36m, line \u001b[1;32m45\u001b[0m\n\u001b[1;33m    for header in table.find('tr').find_all('th'):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def numStr(num):\n",
    "    if num >= 10:\n",
    "        return str(num)\n",
    "    else:\n",
    "        return '0' + str(num)\n",
    "\n",
    "Base = \"http://race.sp.netkeiba.com/?pid=race_result&race_id=\"\n",
    "dst = ''\n",
    "df_col = ['year', 'date', 'field', 'race', 'race_name'\n",
    "          , 'course', 'head_count', 'rank', 'horse_name'\n",
    "          , 'gender', 'age', 'trainerA', 'trainerB', 'weight', 'c_weight', 'jackie', 'j_weight'\n",
    "          , 'odds','popu']\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for year in tqdm(range(2018, 2018)):\n",
    "    for i in tqdm(range(1, 11)):\n",
    "        for j in tqdm(range(1, 11)):\n",
    "            for k in tqdm(range(1, 11)):\n",
    "                for l in range(1, 13):\n",
    "                    # urlでぶっこ抜く\n",
    "                    url = Base + str(year) + numStr(i) + numStr(j) + numStr(k) + numStr(l)\n",
    "                    time.sleep(1)\n",
    "                    html = requests.get(url)\n",
    "                    \n",
    "\n",
    "                    # scraping\n",
    "                    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "                    # ページがあるかの判定\n",
    "                    if soup.find_all('div', attrs={'class', 'Result_Guide'})!=[]:\n",
    "                        break\n",
    "                    else:\n",
    "                        results = []\n",
    "\n",
    "                        res_tables = soup.find_all('table', class_='race_table_01')\n",
    "\n",
    "                        for table in res_tables:\n",
    "                            headers = []\n",
    "                            rows = table.find_all('tr')\n",
    "                                for header in table.find('tr').find_all('th'):\n",
    "                                    headers.append(header.text)\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(result_dict)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フェブラリーＳ\n",
      "ダ1600m(左)\n",
      "天気：晴/馬場：良/発走：15:40\n",
      "{'着順': '1', '枠番': '4', '馬番': '6', '馬名': ' インティ', '性齢': '牡5', '負担重量': '57.0', '騎手': ' 武豊', 'タイム': '1:35.6', '着差': '', '人気': '1', '単勝オッズ': '2.6', '後3F': '35.4', 'コーナー通過順': '1-1', '厩舎': '(栗東)野中', '馬体重': '514(0)'}\n",
      "{'着順': '2', '枠番': '3', '馬番': '3', '馬名': ' ゴールドドリーム', '性齢': '牡6', '負担重量': '57.0', '騎手': ' ルメール', 'タイム': '1:35.6', '着差': 'クビ', '人気': '2', '単勝オッズ': '3.0', '後3F': '34.8', 'コーナー通過順': '6-6', '厩舎': '(栗東)平田', '馬体重': '528(+14)'}\n",
      "{'着順': '3', '枠番': '2', '馬番': '2', '馬名': ' ユラノト', '性齢': '牡5', '負担重量': '57.0', '騎手': ' 福永', 'タイム': '1:36.3', '着差': '4', '人気': '8', '単勝オッズ': '31.6', '後3F': '35.5', 'コーナー通過順': '6-6', '厩舎': '(栗東)松田', '馬体重': '508(+2)'}\n",
      "{'着順': '4', '枠番': '5', '馬番': '8', '馬名': '○外モーニン', '性齢': '牡7', '負担重量': '57.0', '騎手': ' 和田', 'タイム': '1:36.5', '着差': '1.1/4', '人気': '9', '単勝オッズ': '42.9', '後3F': '35.9', 'コーナー通過順': '5-3', '厩舎': '(栗東)石坂正', '馬体重': '518(-4)'}\n",
      "{'着順': '5', '枠番': '7', '馬番': '11', '馬名': '○外コパノキッキング', '性齢': 'セ4', '負担重量': '57.0', '騎手': ' 藤田菜', 'タイム': '1:36.6', '着差': '1/2', '人気': '4', '単勝オッズ': '9.4', '後3F': '35.2', 'コーナー通過順': '13-14', '厩舎': '(栗東)村山', '馬体重': '478(0)'}\n",
      "{'着順': '6', '枠番': '6', '馬番': '10', '馬名': ' サンライズソア', '性齢': '牡5', '負担重量': '57.0', '騎手': ' 田辺', 'タイム': '1:36.7', '着差': 'クビ', '人気': '5', '単勝オッズ': '14.1', '後3F': '36.3', 'コーナー通過順': '2-2', '厩舎': '(栗東)河内', '馬体重': '504(-18)'}\n",
      "{'着順': '7', '枠番': '5', '馬番': '7', '馬名': ' サンライズノヴァ', '性齢': '牡5', '負担重量': '57.0', '騎手': ' 戸崎圭', 'タイム': '1:36.9', '着差': '1.1/4', '人気': '6', '単勝オッズ': '16.7', '後3F': '35.7', 'コーナー通過順': '10-10', '厩舎': '(栗東)音無', '馬体重': '546(+6)'}\n",
      "{'着順': '8', '枠番': '4', '馬番': '5', '馬名': ' サクセスエナジー', '性齢': '牡5', '負担重量': '57.0', '騎手': ' 松山', 'タイム': '1:37.1', '着差': '1', '人気': '11', '単勝オッズ': '95.0', '後3F': '36.5', 'コーナー通過順': '3-3', '厩舎': '(栗東)北出', '馬体重': '542(0)'}\n",
      "{'着順': '9', '枠番': '6', '馬番': '9', '馬名': ' ワンダーリーデル', '性齢': '牡6', '負担重量': '57.0', '騎手': ' 柴田大', 'タイム': '1:37.2', '着差': '1/2', '人気': '14', '単勝オッズ': '429.0', '後3F': '36.6', 'コーナー通過順': '3-3', '厩舎': '(栗東)沖', '馬体重': '530(-4)'}\n",
      "{'着順': '10', '枠番': '8', '馬番': '14', '馬名': ' オメガパフューム', '性齢': '牡4', '負担重量': '57.0', '騎手': ' Ｍデムーロ', 'タイム': '1:37.3', '着差': '3/4', '人気': '3', '単勝オッズ': '6.2', '後3F': '36.4', 'コーナー通過順': '6-8', '厩舎': '(栗東)安田翔', '馬体重': '450(+2)'}\n",
      "{'着順': '11', '枠番': '1', '馬番': '1', '馬名': ' クインズサターン', '性齢': '牡6', '負担重量': '57.0', '騎手': ' 四位', 'タイム': '1:37.4', '着差': '3/4', '人気': '10', '単勝オッズ': '61.3', '後3F': '36.2', 'コーナー通過順': '13-10', '厩舎': '(栗東)野中', '馬体重': '476(0)'}\n",
      "{'着順': '12', '枠番': '7', '馬番': '12', '馬名': ' ノボバカラ', '性齢': '牡7', '負担重量': '57.0', '騎手': ' ミナリク', 'タイム': '1:37.8', '着差': '2', '人気': '12', '単勝オッズ': '338.1', '後3F': '36.6', 'コーナー通過順': '10-10', '厩舎': '(栗東)森', '馬体重': '510(+4)'}\n",
      "{'着順': '13', '枠番': '8', '馬番': '13', '馬名': ' ノンコノユメ', '性齢': 'セ7', '負担重量': '57.0', '騎手': ' 内田博', 'タイム': '1:37.9', '着差': '1/2', '人気': '7', '単勝オッズ': '17.9', '後3F': '36.9', 'コーナー通過順': '6-8', '厩舎': '(美浦)加藤征', '馬体重': '452(+6)'}\n",
      "{'着順': '14', '枠番': '3', '馬番': '4', '馬名': ' メイショウウタゲ', '性齢': '牡8', '負担重量': '57.0', '騎手': ' 北村宏', 'タイム': '1:37.9', '着差': 'クビ', '人気': '13', '単勝オッズ': '393.4', '後3F': '36.7', 'コーナー通過順': '10-10', '厩舎': '(栗東)安達', '馬体重': '520(0)'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#このurlは2019/02/17(日)のフェブラリーステークス(G1)のレース結果です。\n",
    "res = requests.get('https://race.netkeiba.com/?pid=race&id=c201905010811&mode=result')\n",
    "html_doc = res.content\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "#webページの上段を獲得する\n",
    "div_house_detail = soup.find('dl', class_='racedata')\n",
    "\n",
    "#レース名を獲得する\n",
    "race_name = div_house_detail.find('h1')\n",
    "print(race_name.get_text().replace(u\"\\xa0\",u\"\"))\n",
    "\n",
    "#コースの距離と種類を獲得する\n",
    "race_detail = div_house_detail.find('p')\n",
    "print(race_detail.get_text().replace(\"\\xa0\",\"\"))\n",
    "\n",
    "#コースの状態と天気と時間\n",
    "race_detail = div_house_detail.find_all('p')\n",
    "\n",
    "for rd in race_detail:\n",
    "    headers = []\n",
    "    headers.append(rd.text)\n",
    "\n",
    "for header in headers:\n",
    "    print(header.replace(\"\\xa0\",\"\"))\n",
    "\n",
    "#リストを使ってレース結果を保存する\n",
    "results = []\n",
    "\n",
    "res_tables = soup.find_all('table', class_='race_table_01')\n",
    "\n",
    "for table in res_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    #まず、フィールド名を決めるために、最初の行からヘッダーセルを獲得する\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "\n",
    "    #つぎに、1行目以外の行を処理する\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            result_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            results.append(result_dict)\n",
    "#結果を表示\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
